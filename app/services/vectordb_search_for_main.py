import re
from chromadb import PersistentClient
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import torch
from app.services.llm_expand import generate_related_queries  # ç¢ºä¿ä½ å¼•å…¥ LLM æ“´å±•å‡½æ•¸
import psycopg2
import os
from datetime import timedelta
from sklearn.preprocessing import StandardScaler
from dotenv import load_dotenv
load_dotenv()
from app.chroma_client import ChromaDBClient
import google.generativeai as genai
import json  # ç¢ºä¿å·²åŒ¯å…¥


api_key =  os.getenv("GEMINI_API_KEY")

genai.configure(api_key=api_key)

# === åˆå§‹åŒ– ChromaDB client & collection ===
client = ChromaDBClient.get_instance().get_client()
collection_tt = client.get_or_create_collection(name="title_tag_emb")
collection_st = client.get_or_create_collection(name="summary_transcription_emb")
collection_chunks = client.get_or_create_collection(name="transcription_chunks_emb")

# å–å¾—æ‰€æœ‰ collections
collections = client.list_collections()

# === æ¨¡å‹ ===
model_tt = SentenceTransformer("paraphrase-MiniLM-L6-v2", device='cuda' if torch.cuda.is_available() else 'cpu')
model_st = SentenceTransformer("BAAI/bge-m3", device='cuda' if torch.cuda.is_available() else 'cpu')
model = genai.GenerativeModel("gemini-2.5-flash-lite")

# === å…¬ç”¨ cosine similarity å‡½æ•¸ ===
def cosine_similarity(a, b):
    dot = sum(x * y for x, y in zip(a, b))
    norm_a = sum(x * x for x in a) ** 0.5
    norm_b = sum(y * y for y in b) ** 0.5
    return dot / (norm_a * norm_b + 1e-9)

# === æŸ¥è©¢å­—å¹•ç‰‡æ®µæœ€ç›¸é—œçš„èµ·å§‹æ™‚é–“ï¼ˆå–å‰ä¸€æ®µï¼‰ === ###æœ€å¾Œreturnçš„best_idxæœ‰-1æ‰æ˜¯å‰ä¸€æ®µ
def get_best_chunk_start(video_id: str, query: str):
    query_emb = model_st.encode([query])[0]

    # ä½¿ç”¨ collection.query æŸ¥è©¢æœ€ç›¸é—œçš„å‰ 20 å€‹ç‰‡æ®µ
    res = collection_chunks.query(
        query_embeddings=[query_emb],
        n_results=20,
        where={"video_id": video_id},
        include=["metadatas", "distances"]  # åŒ…å«è·é›¢åˆ†æ•¸
    )

    # Step 4: æ•´ç† query çµæœæº–å‚™çµ¦ Gemini é‡æ’åº
    # query() æ–¹æ³•è¿”å›çš„æ˜¯å·¢ç‹€åˆ—è¡¨çµæ§‹ [[...]]
    metadatas = res["metadatas"][0]  # å–ç¬¬ä¸€å€‹æŸ¥è©¢çš„çµæœ
    
    # æº–å‚™çµ¦ Gemini é‡æ’åºçš„è³‡æ–™
    segments_for_rerank = []
    for i, meta in enumerate(metadatas):
        segment_data = {
            "start": meta.get("start", 0),
            "summary": meta.get("summary", ""),
        }
        segments_for_rerank.append(segment_data)
    
    # æº–å‚™çµ¦ Gemini çš„ prompt
    segments_text = ""
    for i, segment in enumerate(segments_for_rerank):
        segments_text += f"ç‰‡æ®µ {i+1}:\n"
        segments_text += f"æ™‚é–“: {segment.get('start', 'N/A')}ç§’\n"
        segments_text += f"å…§å®¹æ‘˜è¦: {segment.get('summary', '')}\n"
        segments_text += f"---\n"
    
    prompt = f"""
è«‹æ ¹æ“šä½¿ç”¨è€…æŸ¥è©¢å°ä»¥ä¸‹å½±ç‰‡ç‰‡æ®µé€²è¡Œç›¸é—œæ€§æ’åºã€‚

ä½¿ç”¨è€…æŸ¥è©¢: "{query}"

å½±ç‰‡ç‰‡æ®µ:
{segments_text}

è«‹ä»”ç´°åˆ†ææ¯å€‹ç‰‡æ®µèˆ‡æŸ¥è©¢çš„ç›¸é—œæ€§ï¼Œè€ƒæ…®ä»¥ä¸‹å› ç´ ï¼š
1. å…§å®¹ä¸»é¡Œçš„åŒ¹é…åº¦
2. é—œéµè©çš„ç›¸é—œæ€§
3. èªç¾©ç›¸ä¼¼æ€§
4. å¯¦ç”¨æ€§å’Œåƒ¹å€¼

è«‹æŒ‰ç…§ç›¸é—œæ€§å¾é«˜åˆ°ä½æ’åºï¼Œä¸¦ä»¥ JSON æ ¼å¼å›å‚³çµæœã€‚æ ¼å¼å¦‚ä¸‹ï¼š
{{
    "ranking": [1, 3, 2, 5, 4, ...],
    "explanation": "ç°¡è¦èªªæ˜æ’åºçš„ç†ç”±"
}}

å…¶ä¸­ ranking æ•¸çµ„åŒ…å«é‡æ–°æ’åºå¾Œçš„ç‰‡æ®µç·¨è™Ÿï¼ˆ1-based indexï¼‰ã€‚
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # å˜—è©¦è§£æ JSON å›æ‡‰
        if result_text.startswith("```json"):
            result_text = result_text[7:-3].strip()
        elif result_text.startswith("```"):
            result_text = result_text[3:-3].strip()
        
        result = json.loads(result_text)
        ranking = result.get("ranking", list(range(1, len(segments_for_rerank) + 1)))
        
        # å–å‡ºç¬¬ä¸€åç‰‡æ®µçš„ start æ™‚é–“
        if ranking:
            top_segment_idx = ranking[0] - 1  # 1-based to 0-based index
            best_segment = segments_for_rerank[top_segment_idx]
            return best_segment["start"]

    except Exception as e:
        print(f"Gemini é‡æ’åºå¤±æ•—: {e}")
        # å¦‚æœ Gemini å¤±æ•—ï¼Œè¿”å›åŸå§‹é †åºçš„ç¬¬ä¸€å€‹ç‰‡æ®µçš„ start
        return segments_for_rerank[0]["start"] if segments_for_rerank else None



# === ä¸»æµç¨‹ ===
def search_videos_with_vectorDB(query: str, k=5):
    expanded_queries = generate_related_queries(query)
    weights = [5, 4, 3, 2, 2, 2]
    final_scores = {}
    need_download = False # é è¨­ä¸éœ€è¦ä¸‹è¼‰
    for i, query_text in enumerate(expanded_queries):
        video_scores = {}
        weight = weights[i]

        q_tt_emb = model_tt.encode([query_text])[0]
        q_st_emb = model_st.encode([query_text])[0]

        # Title å°ˆå±¬å‰ 10
        results_title = collection_tt.query(
            query_embeddings=[q_tt_emb],
            n_results=10,
            where={"field": "title"},
            include=["metadatas", "embeddings"]
        )

        # è™•ç† summary
        results_st = collection_st.query(
            query_embeddings=[q_st_emb],
            n_results=10,
            include=["metadatas", "embeddings"]
        )

        # å»ºç«‹å€™é¸å½±ç‰‡æ¸…å–®ï¼ˆåƒ… title å‘½ä¸­çš„å½±ç‰‡ï¼‰
        candidate_vids = set()
        for emb, meta in zip(results_title["embeddings"][0], results_title["metadatas"][0]):
            vid = meta["video_id"]
            score = (cosine_similarity(q_tt_emb, emb) + 1.0)/2.0
            if score < 0.6:
                score = 0  # éæ¿¾æ‰ä½æ–¼ 0.6 çš„ç›¸ä¼¼åº¦
            video_scores.setdefault(vid, {"title": 0, "tags": 0, "summary": 0})
            video_scores[vid]["title"] = score
            candidate_vids.add(vid)

        # åªç”¨ä¸€å€‹ where æ¢ä»¶ï¼švideo_id in å€™é¸
        tags_docs = collection_tt.get(
            where={"video_id": {"$in": list(candidate_vids)}},  # åªæ”¾ã€Œä¸€å€‹ã€operator
            include=["metadatas", "embeddings"]
        )

        # å°æ¯æ”¯å½±ç‰‡å–ã€Œæ‰€æœ‰ tag å‘é‡ã€çš„æœ€å¤§ç›¸ä¼¼åº¦ç•¶ä½œè©²å½±ç‰‡ tags åˆ†æ•¸
        for emb, meta in zip(tags_docs.get("embeddings", []), tags_docs.get("metadatas", [])):
            if meta.get("field") != "tags":
                continue  # åªç®— tags
            vid = meta.get("video_id")
            if not vid:
                continue
            score = (cosine_similarity(q_tt_emb, emb) + 1.0) / 2.0
            if score < 0.6:
                score = 0.0
            video_scores.setdefault(vid, {"title": 0.0, "tags": 0.0, "summary": 0.0})
            video_scores[vid]["tags"] = max(video_scores[vid]["tags"], score)

        for emb, meta in zip(results_st["embeddings"][0], results_st["metadatas"][0]):
            if meta["field"] != "summary":
                continue
            vid = meta["video_id"]
            score = (cosine_similarity(q_st_emb, emb) + 1.0) / 2.0
            if score < 0.6:
                score = 0  # éæ¿¾æ‰ä½æ–¼ 0.6 çš„ç›¸ä¼¼åº¦
            video_scores.setdefault(vid, {"title": 0, "tags": 0, "summary": 0})
            video_scores[vid]["summary"] = score
        
        for vid, scores in video_scores.items():

            total = (
                0.4 * scores["title"] +
                0.3 * scores["tags"] +
                0.3 * scores["summary"]
            )* weight
            print(f"ä¸»é¡Œ{i}: vid={vid}, title={scores["title"]}, tags={scores["tags"]}, summary={scores["summary"]} => total={total}")
            # å¦‚æœ vid å·²ç¶“å­˜åœ¨ï¼Œå°±ä¿ç•™åˆ†æ•¸æ¯”è¼ƒé«˜çš„
            if vid in final_scores:
                final_scores[vid] = max(final_scores[vid], total)
            else:
                final_scores[vid] = total

    # è½‰æ›æˆ list ä¸¦æ’åº
    sorted_scores = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    top_videos = sorted_scores[:k]
    print("åˆ†æ•¸",sorted_scores[0][1] )
    # âœ… æª¢æŸ¥æœ€é«˜åˆ†æ˜¯å¦å°æ–¼ 2.5
    if not sorted_scores or sorted_scores[0][1] < 2.5:
        print("æŸ¥è©¢ç›¸é—œåº¦æœ€é«˜åˆ†å°æ–¼ 2.5ï¼Œéœ€è¦è‡ªå‹•ä¸‹è¼‰å½±ç‰‡")
        need_download = True
        
    # å–å¾—å½±ç‰‡è³‡è¨Šèˆ‡æ¨è–¦ç‰‡æ®µæ™‚é–“ï¼ˆæ”¹ç”¨ with æ–¹å¼ï¼‰
    # å¦‚æœåˆ†æ•¸ å°æ–¼æ¨™æº–ï¼Œå‰‡éœ€è¦åœ¨èƒŒæ™¯ä¸‹è¼‰å½±ç‰‡
    results = []
    with psycopg2.connect("postgresql://postgres:Functrol@localhost:5432/postgres") as conn:
        with conn.cursor() as cursor:
            for vid , total_score in top_videos:
                # æŠ“ title, summary, embed_url, tag_names
                cursor.execute("""
                    SELECT 
                        v.title, 
                        v.summary, 
                        v.embed_url,
                        ARRAY(
                            SELECT t.name
                            FROM jsonb_array_elements_text(v.tag_ids) AS tag_id_text
                            JOIN tags t ON t.id = tag_id_text::int
                        ) AS tag_names
                    FROM videos v
                    WHERE v.id = %s
                """, (vid,))
                row = cursor.fetchone()
                title, summary, embed_url, tag_names = row if row else ("", "", "", [])

                # æ‰¾æœ€ä½³èµ·å§‹æ™‚é–“
                start = None
                for query_text in expanded_queries:
                    start = get_best_chunk_start(vid, query_text)
                    if start:
                        break

                # å°‡ "HH:MM:SS" è½‰ç‚ºç§’æ•¸
                seconds = 0
                if start:
                    h, m, s = map(float, start.split(":"))
                    seconds = int(h * 3600 + m * 60 + s)

                if embed_url:
                    embed_url = embed_url.split("?")[0] + f"?start={seconds}"

                results.append((total_score, vid, title, summary, embed_url, tag_names))

    return expanded_queries, results, need_download #æ–°å¢ need_download å›å‚³

def search_videos_with_vectorDB_for_map(query: str, k=5):
    expanded_queries = [query]
    weights = [5, 4, 3, 2, 2, 2]
    final_scores = {}
    need_download = False
    for i, query_text in enumerate(expanded_queries):
        video_scores = {}
        weight = weights[i]

        q_tt_emb = model_tt.encode([query_text])[0]
        q_st_emb = model_st.encode([query_text])[0]

        # Title å°ˆå±¬å‰ 10
        results_title = collection_tt.query(
            query_embeddings=[q_tt_emb],
            n_results=10,
            where={"field": "title"},
            include=["metadatas", "embeddings"]
        )

        # è™•ç† summary
        results_st = collection_st.query(
            query_embeddings=[q_st_emb],
            n_results=10,
            include=["metadatas", "embeddings"]
        )

        # å»ºç«‹å€™é¸å½±ç‰‡æ¸…å–®ï¼ˆåƒ… title å‘½ä¸­çš„å½±ç‰‡ï¼‰
        candidate_vids = set()
        for emb, meta in zip(results_title["embeddings"][0], results_title["metadatas"][0]):
            vid = meta["video_id"]
            score = (cosine_similarity(q_tt_emb, emb) + 1.0)/2.0
            if score < 0.6:
                score = 0  # éæ¿¾æ‰ä½æ–¼ 0.6 çš„ç›¸ä¼¼åº¦
            video_scores.setdefault(vid, {"title": 0, "tags": 0, "summary": 0})
            video_scores[vid]["title"] = score
            candidate_vids.add(vid)

        # åªç”¨ä¸€å€‹ where æ¢ä»¶ï¼švideo_id in å€™é¸
        tags_docs = collection_tt.get(
            where={"video_id": {"$in": list(candidate_vids)}},  # åªæ”¾ã€Œä¸€å€‹ã€operator
            include=["metadatas", "embeddings"]
        )

        # å°æ¯æ”¯å½±ç‰‡å–ã€Œæ‰€æœ‰ tag å‘é‡ã€çš„æœ€å¤§ç›¸ä¼¼åº¦ç•¶ä½œè©²å½±ç‰‡ tags åˆ†æ•¸
        for emb, meta in zip(tags_docs.get("embeddings", []), tags_docs.get("metadatas", [])):
            if meta.get("field") != "tags":
                continue  # åªç®— tags
            vid = meta.get("video_id")
            if not vid:
                continue
            score = (cosine_similarity(q_tt_emb, emb) + 1.0) / 2.0
            if score < 0.6:
                score = 0.0
            video_scores.setdefault(vid, {"title": 0.0, "tags": 0.0, "summary": 0.0})
            video_scores[vid]["tags"] = max(video_scores[vid]["tags"], score)

        for emb, meta in zip(results_st["embeddings"][0], results_st["metadatas"][0]):
            if meta["field"] != "summary":
                continue
            vid = meta["video_id"]
            score = (cosine_similarity(q_st_emb, emb) + 1.0) / 2.0
            if score < 0.6:
                score = 0  # éæ¿¾æ‰ä½æ–¼ 0.6 çš„ç›¸ä¼¼åº¦
            video_scores.setdefault(vid, {"title": 0, "tags": 0, "summary": 0})
            video_scores[vid]["summary"] = score
        
        for vid, scores in video_scores.items():

            total = (
                0.4 * scores["title"] +
                0.3 * scores["tags"] +
                0.3 * scores["summary"]
            )* weight
            print(f"ä¸»é¡Œ{i}: vid={vid}, title={scores["title"]}, tags={scores["tags"]}, summary={scores["summary"]} => total={total}")
            # å¦‚æœ vid å·²ç¶“å­˜åœ¨ï¼Œå°±ä¿ç•™åˆ†æ•¸æ¯”è¼ƒé«˜çš„
            if vid in final_scores:
                final_scores[vid] = max(final_scores[vid], total)
            else:
                final_scores[vid] = total

    # è½‰æ›æˆ list ä¸¦æ’åº
    sorted_scores = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    top_videos = sorted_scores[:k]
    print("åˆ†æ•¸",sorted_scores[0][1] )
    if sorted_scores[0][1] < 2.5: #å…ˆçµ¦1ï¼Œå› ç‚ºè¢«YTé–ä½äº†
        need_download = True
        
    # å–å¾—å½±ç‰‡è³‡è¨Šèˆ‡æ¨è–¦ç‰‡æ®µæ™‚é–“ï¼ˆæ”¹ç”¨ with æ–¹å¼ï¼‰
    # å¦‚æœåˆ†æ•¸ å°æ–¼æ¨™æº–ï¼Œå‰‡éœ€è¦åœ¨èƒŒæ™¯ä¸‹è¼‰å½±ç‰‡
    results = []
    with psycopg2.connect("postgresql://postgres:Functrol@localhost:5432/postgres") as conn:
        with conn.cursor() as cursor:
            for vid , total_score in top_videos:
                # æŠ“ title, summary, embed_url, tag_names
                cursor.execute("""
                    SELECT 
                        v.title, 
                        v.summary, 
                        v.embed_url,
                        ARRAY(
                            SELECT t.name
                            FROM jsonb_array_elements_text(v.tag_ids) AS tag_id_text
                            JOIN tags t ON t.id = tag_id_text::int
                        ) AS tag_names
                    FROM videos v
                    WHERE v.id = %s
                """, (vid,))
                row = cursor.fetchone()
                title, summary, embed_url, tag_names = row if row else ("", "", "", [])

                results.append((total_score, vid, title, summary, embed_url, tag_names))

    return expanded_queries, results,need_download


# ç”¨ä¾†æ‰¾ç›®å‰æœ€æ–°çš„id(ç‚ºäº†è‡ªå‹•ä¸‹è¼‰ï¼Œè«‹æŸ¥çœ‹servicesçš„auto_downlaod)
def get_latest_id(): 
    latest_id = 0
    for col in collections:
        name = col.name
        print(f"ğŸ”¸ Collection åç¨±ï¼š{name}")

        collection = client.get_collection(name=name)
        try:
            if name == "tag_vocab":
                break
            data = collection.get(include=["metadatas", "documents"])
            latest_id = data["ids"][-1] if data["ids"] else "ç„¡è³‡æ–™"
            # ä¿ç•™æ•¸å­—
            latest_id = re.sub(r'\D', '', latest_id)
            print(f"   æœ€æ–° IDï¼š{latest_id}")      

        except Exception as e:
            print(f"   âš ï¸ ç„¡æ³•è®€å–è©² collectionï¼š{e}")
    return latest_id
# å„²å­˜è‡ªå‹•æ‘˜è¦(ç‚ºäº†è‡ªå‹•ä¸‹è¼‰)
def parse_time(ts):
    h, m, s = ts.split(":")
    return timedelta(hours=int(h), minutes=int(m), seconds=float(s))

def find_start_index(transcript_json, target_time):
    """å¾å­—å¹•ä¸­æ‰¾åˆ°ç¬¬ä¸€å€‹é–‹å§‹æ™‚é–“ >= target_time çš„ index"""
    for i, seg in enumerate(transcript_json):
        if parse_time(seg["start"]) >= parse_time(target_time):
            return i
    return len(transcript_json)
def slice_transcript_block(transcript_json, start_index, max_tokens=500):
    total_tokens = 0
    block = []
    for i in range(start_index, len(transcript_json)):
        text = transcript_json[i]["content"]
        approx_tokens = len(text) // 4
        if total_tokens + approx_tokens > max_tokens:
            break
        block.append(transcript_json[i])
        total_tokens += approx_tokens
    return block
def gemini_segment_and_summarize(subtitle_slices):
    text_block = ""
    for seg in subtitle_slices:
        text_block += f"[{seg['start']} - {seg['end']}] {seg['content']}\n"

    prompt = f"""
ä½ æ˜¯èªæ„åˆ†æ®µåŠ©æ‰‹ï¼Œè«‹ä¾æ“šä¸‹æ–¹å­—å¹•ï¼ˆåŒ…å«æ™‚é–“èˆ‡å°æ‡‰æ–‡å­—ï¼‰é€²è¡Œåˆ‡æ®µæ‘˜è¦ï¼š
{text_block}

è«‹æ ¹æ“šèªæ„åˆ‡åˆ†é€™äº›å­—å¹•æ®µè½ï¼Œæ¯æ®µéœ€è¦ï¼š
1. é–‹å§‹æ™‚é–“èˆ‡çµæŸæ™‚é–“ï¼ˆä¸èƒ½è¶…å‡ºæä¾›è³‡æ–™ï¼‰
2. æ¯æ®µæ‘˜è¦ï¼ˆ1ï½2 å¥**è‹±æ–‡**ï¼‰

è«‹å›å‚³**ç´” JSON é™£åˆ—**ï¼Œä¸è¦åŠ ä¸Š```jsonæˆ–ä»»ä½•Markdownç¬¦è™Ÿã€‚
æ ¼å¼å¦‚ä¸‹ï¼š
[
  {{"start": "00:00:00", "end": "00:00:05", "summary": "æ®µè½æ‘˜è¦"}}
]
"""
    model_ai = genai.GenerativeModel('gemini-2.5-flash-lite')
    response = model_ai.generate_content(prompt)
    raw_text = response.text if hasattr(response, "text") else response.parts[0].text
    print("ğŸ“¥ Gemini å›å‚³åŸæ–‡ï¼š\n", raw_text[:500])

    # ç”¨ regex æå–ç¬¬ä¸€å€‹åˆæ³•çš„ JSON é™£åˆ—
    json_match = re.search(r"\[\s*{.*?}\s*\]", raw_text, re.DOTALL)
    if not json_match:
        print("âŒ ç„¡æ³•å¾ Gemini å›å‚³ä¸­æå–åˆæ³• JSONã€‚")
        print(raw_text)
        return []

    try:
        return json.loads(json_match.group(0))
    except json.JSONDecodeError as e:
        print("âŒ JSON æ ¼å¼éŒ¯èª¤ï¼š", e)
        print("ğŸ‘‰ éŒ¯èª¤è¼¸å…¥ï¼š\n", json_match.group(0))
        return []

def process_subtitles_with_gemini(video_id, transcript_json, url, title, model_st, collection_chunk):
    current_time = transcript_json[0]["start"]
    existing_ids_chunk = set(collection_chunk.get().get("ids", []))

    while True:
        start_idx = find_start_index(transcript_json, current_time)
        if start_idx >= len(transcript_json):
            break

        block = slice_transcript_block(transcript_json, start_idx)
        if not block:
            break

        try:
            segments = gemini_segment_and_summarize(block)
            if not segments:
                break

            for i, seg in enumerate(segments):
                summary = seg["summary"].strip()
                chunk_id = f"{video_id}_gemini_{seg['start']}_{seg['end']}"
                
                if chunk_id in existing_ids_chunk:
                    continue  # å·²å­˜åœ¨å°±è·³éè©²æ®µ

                vector = model_st.encode([summary])[0].tolist()
                collection_chunk.add(
                    documents=[summary],
                    embeddings=[vector],
                    ids=[chunk_id],
                    metadatas=[{
                        "video_id": video_id,
                        "start": seg["start"],
                        "end": seg["end"],
                        "summary": summary,
                        "url": url,
                        "title": title
                    }]
                )
                print(f"âœ… å„²å­˜æ®µè½ï¼š{chunk_id}")

            # âœ… å„²å­˜å®Œæ‰€æœ‰æ®µè½å¾Œï¼Œå†æª¢æŸ¥æ™‚é–“æœ‰ç„¡å‰é€²
            next_time = segments[-1]["end"]
            if current_time == next_time:
                print(f"âš ï¸ åµæ¸¬åˆ°ç„¡æ™‚é–“å‰é€²ï¼Œè·³å‡ºå¾ªç’°ï¼š{current_time}")
                break
            current_time = next_time

        except Exception as e:
            print(f"âŒ Gemini åˆ†æéŒ¯èª¤ï¼švideo_id={video_id}ï¼ŒéŒ¯èª¤ï¼š{e}")
            break

# è‡ªå‹•ä¸‹è¼‰æ‘˜è¦è™•ç†(servicesçš„auto_downloadæœ‰ç”¨åˆ°ï¼Œç›®å‰å•é¡Œ:collection_vocabé¡¯ç¤ºæ²’æ±è¥¿)
def store_emb(latest_id, conn):
    cursor = conn.cursor()

    # === æŠ“å½±ç‰‡åŸºæœ¬è³‡æ–™ + å­—å¹• + tags é™£åˆ— ===
    cursor.execute("""
        SELECT
            v.id AS video_id,
            v.url,
            v.title,
            v.summary,
            v.transcription,
            v.transcription_with_time,
            (
                SELECT ARRAY_AGG(t.name ORDER BY t.name)
                FROM jsonb_array_elements_text(v.tag_ids) AS tag_id_text
                JOIN tags t ON t.id = tag_id_text::int
            ) AS tags_arr
        FROM video_categories vc
        JOIN videos v ON v.id = vc.video_id
        JOIN categories c ON vc.category_id = c.id
        WHERE v.transcription_with_time IS NOT NULL AND v.id > %s
        GROUP BY v.id, v.url, v.title, v.summary, v.transcription, v.transcription_with_time, v.tag_ids
        ORDER BY v.id
    """, (latest_id,))
    rows = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]

    # === å·²å­˜åœ¨çš„ IDï¼ˆé¿å…é‡è¤‡ï¼‰ ===
    existing_ids_tt = set(collection_tt.get().get("ids", []))
    existing_ids_st = set(collection_st.get().get("ids", []))
    existing_ids_chunk = set(collection_chunks.get().get("ids", []))

    # === æ¬„ä½å°æ‡‰æ¨¡å‹èˆ‡ collectionï¼ˆä¸å†æŠŠåˆä½µ tags ç•¶ä¸€é¡†å­˜å…¥ï¼‰ ===
    field_mapping = {
        "title": (collection_tt, model_tt),
        "summary": (collection_st, model_st),
        "transcription": (collection_st, model_st),
    }

    for row in rows:
        row_dict = dict(zip(columns, row))
        video_id = str(row_dict["video_id"])
        url = row_dict["url"]
        title = row_dict["title"] or ""
        t_with_time = row_dict["transcription_with_time"]
        tags_arr = [t.strip() for t in (row_dict.get("tags_arr") or []) if t and t.strip()]

        # === å„²å­˜ title/summary/transcription å‘é‡ ===
        for field, (collection, model) in field_mapping.items():
            content = (row_dict.get(field) or "").strip()
            if not content:
                continue

            uid = f"{video_id}_{field}"
            if (collection is collection_tt and uid in existing_ids_tt) or \
               (collection is collection_st and uid in existing_ids_st):
                print(f"âš ï¸ å·²å­˜åœ¨ï¼š{uid}ï¼Œè·³éå„²å­˜")
                continue

            vector = model.encode([content])[0].tolist()
            collection.add(
                documents=[content],
                embeddings=[vector],
                ids=[uid],
                metadatas=[{
                    "video_id": video_id,
                    "field": field,
                    "url": url,
                    "title": title,
                    "kind": "single" if field == "title" else "doc"
                }]
            )

        # === æ¯é¡† tag å„å­˜ä¸€ç­† ===
        for idx, tag_text in enumerate(tags_arr):
            uid = f"{video_id}_tag_{idx}"
            if uid in existing_ids_tt:
                # å…è¨±ä½ æ—¥å¾Œæ›´æ–°ç­–ç•¥æ™‚é¿é–‹é‡è¤‡
                continue
            vec = model_tt.encode([tag_text])[0].tolist()
            collection_tt.add(
                documents=[tag_text],
                embeddings=[vec],
                ids=[uid],
                metadatas=[{
                    "video_id": video_id,
                    "field": "tags",
                    "kind": "single",     # ä¹‹å¾ŒæŸ¥è©¢å¯ where={"field":"tags","kind":"single"}
                    "tag": tag_text,
                    "url": url,
                    "title": title
                }]
            )

        # === è™•ç† Gemini å­—å¹•åˆ†æ®µæ‘˜è¦ ===
        try:
            process_subtitles_with_gemini(
                video_id=video_id,
                transcript_json=t_with_time,
                url=url,
                title=title,
                model_st=model_st,
                collection_chunk=collection_chunks
            )
        except Exception as e:
            print(f"âŒ è™•ç† Gemini åˆ†æ®µå¤±æ•—ï¼švideo_id={video_id}ï¼ŒéŒ¯èª¤ï¼š{e}")

    print("âœ… å·²æˆåŠŸå°‡æ‰€æœ‰å½±ç‰‡æ¬„ä½èˆ‡ã€æ¯é¡† tagã€çš„å‘é‡å„²å­˜å®Œæˆï¼")
    cursor.close()   
